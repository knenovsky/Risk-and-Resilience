train_data <- Xgboostinput_filtered %>% filter(Country.Code%in% train_countries)
test_data <- Xgboostinput_filtered %>% filter(!(Country.Code%in% train_countries))
train_data$group <- as.numeric(factor(train_data$Country.Code))
# Set up trainControl to perform group-wise cross-validation
folds<-createFolds(train_data$group, k = 5, list = TRUE)
custom_control <- trainControl(
index = folds,
method = "repeatedcv",
number = 10,
repeats = 5, # Number of repetitions
verboseIter = TRUE,allowParallel = F
)
param_grid <- expand.grid(
nrounds = c(100, 200, 300),  # Number of boosting rounds
max_depth = c(3, 4, 5),     # Maximum tree depth
eta = c(0.01, 0.1, 0.3),    # Learning rate
gamma = c(0, 1, 10),       # L2 regularization
colsample_bytree = c( 0.9,1),
subsample = c(0.7, 0.8, 0.9),  # Subsample ratio
min_child_weight = c(1, 5, 10)  # Minimum child weight
)
Sys.setenv(OPENBLAS_NUM_THREADS = 1)
doMC::registerDoMC(1)
library(doParallel)
stopCluster(cluster)
df_scaled<-Xgboostinput_iso[,-1] %>% scale() %>% data.frame()
df_scaled_withlabels<-cbind(Xgboostinput_iso[,1],Xgboostinput_iso[,-1])
colnames(df_scaled_withlabels)<-colnames(Xgboostinput_iso)
Xgboostinput_filtered<-df_scaled_withlabels%>%  filter(!(Country.Code%in% "PER"))
set.seed(topruns$SeedNum[1])
features_iso<-Xgboostinput_filtered %>%  select(-p_value_mort,-Country.Code) %>%  colnames()
features_iso<-features_iso
wdi_imputed_shaptable<-data.frame()
wdi_imputed_bootstrap<-data.frame()
# Split test training
countries<-Xgboostinput_filtered$Country.Code %>%  unique
test_proportion <- 0.8
train_countries <- sample(countries, size = length(countries) * test_proportion)
train_data <- Xgboostinput_filtered %>% filter(Country.Code%in% train_countries)
test_data <- Xgboostinput_filtered %>% filter(!(Country.Code%in% train_countries))
train_data$group <- as.numeric(factor(train_data$Country.Code))
# Set up trainControl to perform group-wise cross-validation
folds<-createFolds(train_data$group, k = 5, list = TRUE)
custom_control <- trainControl(
index = folds,
method = "repeatedcv",
number = 10,
repeats = 5, # Number of repetitions
verboseIter = TRUE,allowParallel = F
)
param_grid <- expand.grid(
nrounds = c(100, 200, 300),  # Number of boosting rounds
max_depth = c(3, 4, 5),     # Maximum tree depth
eta = c(0.01, 0.1, 0.3),    # Learning rate
gamma = c(0, 1, 10),       # L2 regularization
colsample_bytree = c( 0.9,1),
subsample = c(0.7, 0.8, 0.9),  # Subsample ratio
min_child_weight = c(1, 5, 10)  # Minimum child weight
)
Sys.setenv(OPENBLAS_NUM_THREADS = 1)
doMC::registerDoMC(1)
library(doParallel)
stopCluster(cluster)
df_scaled<-Xgboostinput_iso[,-1] %>% scale() %>% data.frame()
df_scaled_withlabels<-cbind(Xgboostinput_iso[,1],Xgboostinput_iso[,-1])
colnames(df_scaled_withlabels)<-colnames(Xgboostinput_iso)
Xgboostinput_filtered<-df_scaled_withlabels%>%  filter(!(Country.Code%in% "PER"))
set.seed(topruns$SeedNum[1])
features_iso<-Xgboostinput_filtered %>%  select(-p_value_mort,-Country.Code) %>%  colnames()
features_iso<-features_iso
wdi_imputed_shaptable<-data.frame()
wdi_imputed_bootstrap<-data.frame()
# Split test training
countries<-Xgboostinput_filtered$Country.Code %>%  unique
test_proportion <- 0.8
train_countries <- sample(countries, size = length(countries) * test_proportion)
train_data <- Xgboostinput_filtered %>% filter(Country.Code%in% train_countries)
test_data <- Xgboostinput_filtered %>% filter(!(Country.Code%in% train_countries))
train_data$group <- as.numeric(factor(train_data$Country.Code))
# Set up trainControl to perform group-wise cross-validation
folds<-createFolds(train_data$group, k = 5, list = TRUE)
custom_control <- trainControl(
index = folds,
method = "repeatedcv",
number = 10,
repeats = 5, # Number of repetitions
verboseIter = TRUE,allowParallel = F
)
param_grid <- expand.grid(
nrounds = c(100, 200, 300),  # Number of boosting rounds
max_depth = c(3, 4, 5),     # Maximum tree depth
eta = c(0.01, 0.1, 0.3),    # Learning rate
gamma = c(0, 1, 10),       # L2 regularization
colsample_bytree = c( 0.9,1),
subsample = c(0.7, 0.8, 0.9),  # Subsample ratio
min_child_weight = c(1, 5, 10)  # Minimum child weight
)
Sys.setenv(OPENBLAS_NUM_THREADS = 1)
doMC::registerDoMC(1)
library(doParallel)
cluster <- makeForkCluster(30)
registerDoParallel(cluster)
# Test different parameters
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
#tuneGrid = param_grid,
#trControl = custom_control,
#tuneLength = 1000,
verbosity = 1
)
df_scaled<-Xgboostinput_iso[,-1] %>% scale() %>% data.frame()
df_scaled_withlabels<-cbind(Xgboostinput_iso[,1],Xgboostinput_iso[,-1])
colnames(df_scaled_withlabels)<-colnames(Xgboostinput_iso)
Xgboostinput_filtered<-df_scaled_withlabels%>%  filter(!(Country.Code%in% "PER"))
set.seed(topruns$SeedNum[1])
features_iso<-Xgboostinput_filtered %>%  select(-p_value_mort,-Country.Code) %>%  colnames()
wdi_imputed_shaptable<-data.frame()
wdi_imputed_bootstrap<-data.frame()
set.seed(1212)
# Split test training
countries<-Xgboostinput_filtered$Country.Code %>%  unique
test_proportion <- 1
train_countries <- sample(countries, size = length(countries) * test_proportion)
train_data <- Xgboostinput_filtered %>% filter(Country.Code%in% train_countries)
train_data$group <- as.numeric(factor(train_data$Country.Code))
folds<-createFolds(train_data$group, k = 5, list = TRUE)
# Load the necessary libraries
library(doParallel)
library(caret)
# Register 30 cores for parallel processing
modeliso10$Xgboost_summary->resultsEval
resultsEval$trainR2 %>% mean
set.seed(627)
# Custom trainControl with repeated cross-validation
custom_control <- trainControl(
index = folds,
method = "repeatedcv",
number = 10,
repeats = 5,  # Number of repetitions
verboseIter = TRUE,
allowParallel = TRUE
)
# Parameter grid for XGBoost
param_grid <- expand.grid(
nrounds = c(300),  # Number of boosting rounds
max_depth = c( 3),     # Maximum tree depth
eta = c(0.01),    # Learning rate
gamma = c(1),       # L2 regularization
colsample_bytree = c( 0.8),
subsample = c(0.8),  # Subsample ratio
min_child_weight = c(5)
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
stopCluster(cluster)
# Load the necessary libraries
library(doParallel)
library(caret)
# Register 30 cores for parallel processing
modeliso10$Xgboost_summary->resultsEval
resultsEval$trainR2 %>% mean
set.seed(627)
# Custom trainControl with repeated cross-validation
custom_control <- trainControl(
index = folds,
method = "repeatedcv",
number = 10,
repeats = 5,  # Number of repetitions
verboseIter = TRUE,
allowParallel = TRUE
)
# Parameter grid for XGBoost
param_grid <- expand.grid(
nrounds = c(300),  # Number of boosting rounds
max_depth = c( 3),     # Maximum tree depth
eta = c(0.01),    # Learning rate
gamma = c(1),       # L2 regularization
colsample_bytree = c( 0.8),
subsample = c(0.8),  # Subsample ratio
min_child_weight = c(5)
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
rm(list = ls())
# devtools::install_github("BlakeRMills/MetBrewer")
libraries <- c("magrittr", "ggplot2", "data.table","patchwork","tidygraph","ggraph","igraph","tidyverse",
"ggcorrplot","MetBrewer","grid","gridExtra","ggpubr","cowplot","colorspace","RColorBrewer","gt","gtExtras",  "parallel","readxl","caret","doMC","xgboost","energy","sf")
for (lib in libraries) {
if (!require(lib, character.only = TRUE)) {
install.packages(lib)
library(lib, character.only = TRUE)
}
}
#Theme settings
## plot theme
# color_seven<- ggokabeito::palette_okabe_ito()
col_pal<- brewer.pal(n = 8, name = "Dark2")
options(
# set default colors in ggplot2 to colorblind-friendly
# Okabe-Ito and Viridis palettes
ggplot2.discrete.colour = "viridis",
ggplot2.discrete.fill = "viridis",
ggplot2.continuous.colour = "viridis",
ggplot2.continuous.fill = "viridis",
# set theme font and size
book.base_family = "sans",
book.base_size = 8
)
# Gradient for 0 to 100 viridis_b
#Cassatt1 or OKeeffe1 Colorblindfrinedly gradient form blue to red
#Kandinsky four colors
#Veronese 7 colors
## set default theme
theme_set(
theme_bw(
base_size = getOption("book.base_size"),
base_family = getOption("book.base_family")
) %+replace%
theme(
panel.grid.minor = element_blank(),
legend.position = "bottom"
)
)
detach("package:MASS", unload = TRUE)
#Cluster
#Dir path
DATA_DIR <- "../data/04_data"
FIGURE_DIR <- "../figures/04_figures"
LIB_DIR<-"../libraries"
WDI_DIR <- "../data/231103_WDI"
DIMRED_DIR<-"../data/01_data"
EXMORT_DIR <- "../data/230225_WHO_Excess_Mort/2022-03-25_covid-19_gem"
OWID_DIR<-"../data/231103_OWID_COVID"
OUTPUT_DIR<- "../output"
# File path
exmort_path <- file.path(EXMORT_DIR, "WHO_COVID_Excess_Deaths_EstimatesByCountry.xlsx")
dimred_path<-file.path(DIMRED_DIR,"01_wdi_data_cons_df_250.csv")
wdi_series_path <- file.path(WDI_DIR, "WDISeries.csv")
wdi_gf_path<-file.path(DIMRED_DIR, "01_wdi_gapfilled.csv")
toolbox_path <- file.path(LIB_DIR, "00_toolbox.R") # Source helper functions
owid_series_path<-file.path(OWID_DIR,"231103owid-covid-data.csv")
##Path to trained feature spaces (trained + Summarized 02)
raw_wdi_only_path<-file.path(OUTPUT_DIR,"../output/WDI_scaled_randomgrid_raw.RDS")
imputed_wdi_only_path<-file.path(OUTPUT_DIR,"WDI_randomgrid_scaled_imputed.RDS")
pc20_wdi_only_path<-file.path(OUTPUT_DIR,"WDI_randomgrid_scaled_pc20.RDS")
iso10_wdi_only_path<-file.path(OUTPUT_DIR,"WDI_randomgrid_scaled_iso10.RDS")
## Final model plus covariates (trained + Summarized 03)
pc20_model_path<-file.path(OUTPUT_DIR,"03_Xgboost_pca20_models")
iso10_model_path<-file.path(OUTPUT_DIR,"03_Xgboost_E_iso10_models")
# iso5_model_path<-file.path(OUTPUT_DIR,"03_Xgboost_iso_models")
## Datasets for final analysis
pca20_final_df_path<-paste0("../data/03_data/XboostDfPCA.RDS")
iso10_final_df_path<-paste0("../data/03_data/XboostDfiso.RDS")
# Dimred contstruction data
RELATIVE_INDICATORS_FILE_NAME <- "../data/01_data/relative_indicators_2023_06.R"
wdi_data_cons_gf<-fread("../data/01_data/01_wdi_gapfilled.csv")
loading_df<-readRDS("../data/01_data/01_wdi_loadings.Rds") %>% data.frame()
#Cluster
blas_set_num_threads(30)
omp_set_num_threads(30)
#Dir path
DATA_DIR <- "../data/04_data"
FIGURE_DIR <- "../figures/04_figures"
LIB_DIR<-"../libraries"
WDI_DIR <- "../data/231103_WDI"
DIMRED_DIR<-"../data/01_data"
EXMORT_DIR <- "../data/230225_WHO_Excess_Mort/2022-03-25_covid-19_gem"
OWID_DIR<-"../data/231103_OWID_COVID"
OUTPUT_DIR<- "../output"
# File path
exmort_path <- file.path(EXMORT_DIR, "WHO_COVID_Excess_Deaths_EstimatesByCountry.xlsx")
dimred_path<-file.path(DIMRED_DIR,"01_wdi_data_cons_df_250.csv")
wdi_series_path <- file.path(WDI_DIR, "WDISeries.csv")
wdi_gf_path<-file.path(DIMRED_DIR, "01_wdi_gapfilled.csv")
toolbox_path <- file.path(LIB_DIR, "00_toolbox.R") # Source helper functions
owid_series_path<-file.path(OWID_DIR,"231103owid-covid-data.csv")
##Path to trained feature spaces (trained + Summarized 02)
raw_wdi_only_path<-file.path(OUTPUT_DIR,"../output/WDI_scaled_randomgrid_raw.RDS")
imputed_wdi_only_path<-file.path(OUTPUT_DIR,"WDI_randomgrid_scaled_imputed.RDS")
pc20_wdi_only_path<-file.path(OUTPUT_DIR,"WDI_randomgrid_scaled_pc20.RDS")
iso10_wdi_only_path<-file.path(OUTPUT_DIR,"WDI_randomgrid_scaled_iso10.RDS")
## Final model plus covariates (trained + Summarized 03)
pc20_model_path<-file.path(OUTPUT_DIR,"03_Xgboost_pca20_models")
iso10_model_path<-file.path(OUTPUT_DIR,"03_Xgboost_E_iso10_models")
# iso5_model_path<-file.path(OUTPUT_DIR,"03_Xgboost_iso_models")
## Datasets for final analysis
pca20_final_df_path<-paste0("../data/03_data/XboostDfPCA.RDS")
iso10_final_df_path<-paste0("../data/03_data/XboostDfiso.RDS")
# Dimred contstruction data
RELATIVE_INDICATORS_FILE_NAME <- "../data/01_data/relative_indicators_2023_06.R"
wdi_data_cons_gf<-fread("../data/01_data/01_wdi_gapfilled.csv")
loading_df<-readRDS("../data/01_data/01_wdi_loadings.Rds") %>% data.frame()
## Excess Mort
ExcessMortbyCountry<-readxl::read_excel(exmort_path,sheet = 2)
excessCounrty_annual<-ExcessMortbyCountry %>% group_by(country,iso3,year) %>%
reframe(p_value_mort=sum(excess.mean)/sum(expected.mean)*100,ACM=sum(acm.mean))
excessCounrty_annual2 <- dplyr::mutate(excessCounrty_annual, identifier = paste0(iso3, year),"Country Code"=iso3) %>% select(-iso3)
## Dimred Data
dimred_250<-fread(dimred_path)
dimred_250$iso1 <- -dimred_250$iso1
# Flip the dimensions for other analysis
dimred_250$PC_normal_1 <- -dimred_250$PC_normal_1
dimred_250$PC_normal_5 <- -dimred_250$PC_normal_5
dimred_250$PC_normal_14 <- -dimred_250$PC_normal_14
dimred_250$PC_normal_6 <- -dimred_250$PC_normal_6
dimred_250$PC_normal_23 <- -dimred_250$PC_normal_23
## WDI
wdi_series <- fread(wdi_series_path) %>%
setnames("Series Code", "Indicator Code")
##
# ## PCA best model summary
# summaryPCA_models<-readRDS(pc26_model_path)
# ## ISO best model summary
# summaryISO_models<-readRDS(iso10_model_path)
require("rnaturalearth")
require(rnaturalearthdata)
eck <- "+proj=eck4"
world <- ne_countries(scale = "medium", returnclass = "sf")  %>%
st_wrap_dateline(options = c("WRAPDATELINE=YES","DATELINEOFFSET=180"), quiet = TRUE) %>%
st_transform(crs = eck)
Xgboostinput_complete_iso <-readRDS("../data/03_data/XboostDfEiso.RDS")
# Xgboostinput_complete_iso %>% fwrite("../output/04_final_dataset.csv")
Xgboostinput_iso<-Xgboostinput_complete_iso%>% rename("Country.Code"=`Country Code`,
"Diabetes (%)" =`Diabetetes (%)`,
"COVID-19 cases"=`Covid cases per million`,
"Comorbity"=`Comorbidity index` )
Label<-"p_value_mort"
features<-Xgboostinput_iso %>% select(-`Country.Code`,-p_value_mort) %>%  colnames()
features_iso<-Xgboostinput_iso %>% select(-`Country.Code`,-p_value_mort) %>%  colnames()
features_iso<-features_iso
df_scaled<-Xgboostinput_iso[,-1] %>% scale() %>% data.frame()
df_scaled_withlabels<-cbind(Xgboostinput_iso[,1],Xgboostinput_iso[,-1])
colnames(df_scaled_withlabels)<-colnames(Xgboostinput_iso)
Xgboostinput_filtered<-df_scaled_withlabels%>%  filter(!(Country.Code%in% "PER"))
set.seed(topruns$SeedNum[1])
modeliso10<-readRDS("../output/240822_archived/03_Xgboost_E_iso10_models")
modeliso10$Feature_summary$feature %>%  unique
topruns<-modeliso10$Xgboost_summary %>% arrange(desc(Rsquared))
modeliso10$Xgboost_summary$Rsquared %>%  max()
modeliso10$Xgboost_summary$Rsquared %>%  median
xgboostresults<-modeliso10$Xgboost_summary
# Load the necessary libraries
library(doParallel)
library(caret)
# Register 30 cores for parallel processing
modeliso10$Xgboost_summary->resultsEval
resultsEval$trainR2 %>% mean
set.seed(627)
# Custom trainControl with repeated cross-validation
custom_control <- trainControl(
index = folds,
method = "repeatedcv",
number = 10,
repeats = 5,  # Number of repetitions
verboseIter = TRUE,
allowParallel = TRUE
)
df_scaled<-Xgboostinput_iso[,-1] %>% scale() %>% data.frame()
df_scaled_withlabels<-cbind(Xgboostinput_iso[,1],Xgboostinput_iso[,-1])
colnames(df_scaled_withlabels)<-colnames(Xgboostinput_iso)
Xgboostinput_filtered<-df_scaled_withlabels%>%  filter(!(Country.Code%in% "PER"))
set.seed(topruns$SeedNum[1])
features_iso<-Xgboostinput_filtered %>%  select(-p_value_mort,-Country.Code) %>%  colnames()
wdi_imputed_shaptable<-data.frame()
wdi_imputed_bootstrap<-data.frame()
set.seed(1212)
# Split test training
countries<-Xgboostinput_filtered$Country.Code %>%  unique
test_proportion <- 1
train_countries <- sample(countries, size = length(countries) * test_proportion)
train_data <- Xgboostinput_filtered %>% filter(Country.Code%in% train_countries)
train_data$group <- as.numeric(factor(train_data$Country.Code))
folds<-createFolds(train_data$group, k = 5, list = TRUE)
# Load the necessary libraries
library(doParallel)
library(caret)
# Register 30 cores for parallel processing
modeliso10$Xgboost_summary->resultsEval
resultsEval$trainR2 %>% mean
set.seed(627)
# Custom trainControl with repeated cross-validation
custom_control <- trainControl(
index = folds,
method = "repeatedcv",
number = 10,
repeats = 5,  # Number of repetitions
verboseIter = TRUE,
allowParallel = TRUE
)
# Parameter grid for XGBoost
param_grid <- expand.grid(
nrounds = c(300),  # Number of boosting rounds
max_depth = c( 3),     # Maximum tree depth
eta = c(0.01),    # Learning rate
gamma = c(1),       # L2 regularization
colsample_bytree = c( 0.8),
subsample = c(0.8),  # Subsample ratio
min_child_weight = c(5)
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
View(train_data)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
# Load the necessary libraries
library(doParallel)
library(caret)
# Register 30 cores for parallel processing
modeliso10$Xgboost_summary->resultsEval
resultsEval$trainR2 %>% mean
set.seed(627)
# Custom trainControl with repeated cross-validation
custom_control <- trainControl(
index = folds,
method = "repeatedcv",
number = 10,
repeats = 5,  # Number of repetitions
verboseIter = TRUE,
allowParallel = TRUE
)
# Parameter grid for XGBoost
param_grid <- expand.grid(
nrounds = c(300),  # Number of boosting rounds
max_depth = c( 3),     # Maximum tree depth
eta = c(0.01),    # Learning rate
gamma = c(1),       # L2 regularization
colsample_bytree = c( 0.8),
subsample = c(0.8),  # Subsample ratio
min_child_weight = c(5)
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
tuneGrid = param_grid,
trControl = custom_control,
verbosity = 1
)
# Train the model using caret with parallel processing
xgb_caret_wdi_raw <- caret::train(
x = train_data %>% dplyr::select(features_iso),
y = train_data %>% pull(Label),
method = "xgbTree",
verbosity = 1
)
