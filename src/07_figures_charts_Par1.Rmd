---
title: "07_figures_charts"
author: "Kolja Nenoff"
date: "2023-07-08"
output: html_document
---
```{r setup, include=FALSE}
#rm(list = ls())
library(rstudioapi)
library(magrittr)
library(tidyverse)
library(data.table)
library(parallel)
library(dimRed)
library(devtools)
library(igraph)
library(energy)
library(ggrepel)
library(ggalt)
library(ggraph)
library(trend)
library(riverplot)
library(RANN)
library(RJSONIO)
library(RSpectra)
library(pcaMethods)
require(broom)
require(GGally)
require(ggpubr)
require(xgboost)
require(caret)
require(pdp)
require(iml)
require(data.table)
require(tidyverse)
require(cowplot)
require(readxl)
```
```{r}
wdi_explained<-fread("../data/WDI_data_230609/WDI_CSV/WDISeries.csv")
#load("../data/07_230504_OWID_dataset")
dimred2021_knn250_2<-fread("../output/02_230612_Dimred_ExcessMort_annual.csv")
col_pal <- c("#1F968B",  "#7570b3", "#e7298a", "#3CBB75","yellow3", "orange","slategrey")
```
# functions
```{r}
source("00_toolbox.R")
source("../libraries/01a_lib_social_indicators.R")

```

# data
```{r,Excess Mortility WHO Set}

excessCounrty_year<-read_excel("../../01_data_prep/data/set_mortality_230221/230224_nature_WHO_excess_death/2022-03-25_covid-19_gem/WHO_COVID_Excess_Deaths_EstimatesByCountry.xlsx",sheet = 2)
excessCounrty_month<-read_excel("../../01_data_prep/data/set_mortality_230221/230224_nature_WHO_excess_death/2022-03-25_covid-19_gem/WHO_COVID_Excess_Deaths_EstimatesByCountry.xlsx",sheet = 2)
excessCounrty_monthly_pvalue<-excessCounrty_year  %>% 
  mutate(p_value_mort=(excess.mean)/(expected.mean)*100)
# excessCounrty_covariates<-read_excel("../data/230224_nature_WHO_excess_death/2022-03-25_covid-19_gem/WHO_COVID_Excess_Deaths_EstimatesByCountry.xlsx",sheet = 3)
# excessRegion_year<-read_excel("../data/230224_nature_WHO_excess_death/2022-03-25_covid-19_gem/WHO_COVID_Excess_Deaths_EstimatesByRegion.xlsx",sheet = 2)
# excessRegion_income<-read_excel("../data/230224_nature_WHO_excess_death/2022-03-25_covid-19_gem/WHO_COVID_Excess_Deaths_EstimatesByIncome.xlsx",sheet = 2)
# excessRegion_income %>% group_by(income) %>% 
#   summarise(p_value_mort=sum(excess.mean)/sum(expected.mean)*100,excess_comul=sum(excess.mean))
excessCounrty_year_pvalue<-excessCounrty_year %>% group_by(country,iso3,year) %>% 
  summarise(p_value_mort=sum(excess.mean)/sum(expected.mean)*100,excess_comul=sum(excess.mean))
excessCounrty_year %>% filter(year==2020)-> excessCounrty_year_2020
p_value_global_2020=sum(excessCounrty_year_2020$excess.mean,na.rm = T)/sum(excessCounrty_year_2020$expected.mean,na.rm = T)*100
excessCounrty_year %>% filter(year==2021)-> excessCounrty_year_2020
p_value_global_2021=sum(excessCounrty_year_2020$excess.mean,na.rm = T)/sum(excessCounrty_year_2020$expected.mean,na.rm = T)*100
excessCounrty_monthly_pvalue<-excessCounrty_year  %>% 
  mutate(p_value_mort=(excess.mean)/(expected.mean)*100)
# 
# excessmonthly <- excessCounrty_monthly_pvalue %>% arrange(country,year,month) %>% rename(`Country Code`=iso3) %>% 
#   mutate(date = as.Date(paste(year, month, "01", sep = "-"), format = "%Y-%m-%d"),identifier=paste0(`Country Code`,"_",year(date),"_",month(date)))%>% filter(date>"2020-02-01") %>% merge( . , thisisOWID,by.x="identifier",by.y="monthyear")
```


# Method 1 Compare ISO and PCA
```{r}
## set up path
DATA_DIR <- "../data/"
FIGURE_DIR <- "../figures/01_figures/"
WDI_DIR <- paste0(DATA_DIR, "WDI_data_230609/WDI_CSV")
RELATIVE_INDICATORS_FILE_NAME <- "01_data/relative_indicators_2023_06.R" # nolint
source("../libraries/01a_lib_social_indicators.R")


residual_sample_variances <- readRDS("../data/residual_sample_variances.RDS")
cons_iso_pca<-readRDS("../data/cons_iso_pca.RDS")
occurrence_table       <- cons_iso_pca$occurrence_table
wdi_data_cons          <- cons_iso_pca$wdi_data_cons
wdi_data_cons_pca      <- cons_iso_pca$wdi_data_cons_pca
wdi_data_cons_iso      <- cons_iso_pca$wdi_data_cons_iso
wdi_data_qual_cons_pca <- cons_iso_pca$wdi_data_qual_cons_pca
wdi_data_qual_cons_iso <- cons_iso_pca$wdi_data_qual_cons_iso
knns                   <- cons_iso_pca$knns
knns
cons_iso_pca$wdi_data_cons_iso %>%  head
wdi_data_qual_cons_iso
```


```{r}
pca_res <- residual_sample_variances$pca
iso_res <- residual_sample_variances$iso
median_res_var_pca <- residual_sample_variances$median_pca
median_res_var_iso <- residual_sample_variances$median_iso
hdi_res_var <- residual_sample_variances$hdi

plot_if_no_file(
  get_plot_file("01_230708_pca_iso_res_var.pdf"),
  plot_residual_variances,
  residual_sample_variances = residual_sample_variances,
  wdi_data_qual_cons_iso = wdi_data_qual_cons_iso,
  wdi_data_qual_cons_pca = wdi_data_qual_cons_pca,
  .overwrite = TRUE
)
```
```{r}
residual_sample_variances = residual_sample_variances
wdi_data_qual_cons_pca = wdi_data_qual_cons_pca
wdi_data_qual_cons_iso = wdi_data_qual_cons_iso
  
old_par <- par()
par(mar = c(3.5, 3.75, 0, 0) + 0.1, las = 1, mgp = c(2.5, 0.25, 0))
on.exit(par(old_par))

max_x <- 14

pca_res <- residual_sample_variances$pca
iso_res <- residual_sample_variances$iso
median_res_var_pca <- residual_sample_variances$median_pca
median_res_var_iso <- residual_sample_variances$median_iso
hdi_res_var <- residual_sample_variances$hdi

max_res_var_1 <- 1
png("../figures/06_plots/myplot.png",width = 720,height = 720,units = "px")
plot(pca_res[[1]],
     ylim = c(0, max_res_var_1),
     ## xlim = c(1, max_dim),
     xlim = c(1, max_x),
     ylab = "residual variance",
     xlab = "number of dimensions",
     cex.lab = 2,
     las = 1,
     xaxt = "n",
     yaxt = "n",
     bty = "n",
     type = "n")
points(wdi_data_qual_cons_pca,      type = "b",
       lwd = 4, col = lighten(col_pal[7], 0.8))
points(wdi_data_qual_cons_iso[[2]], type = "b",
       lwd = 4, col = lighten(col_pal[6], 0.8))
text(1, wdi_data_qual_cons_pca[1],      "PCA",
     pos = 4, offset = 0.5, xpd = TRUE, cex = 2)
text(1, wdi_data_qual_cons_iso[[3]][1], "Ensemble Isomap",
     pos = 4, offset = 0.5, xpd = TRUE, cex = 2)
text(max_x + 0.75,         0.1, "10% Residual Variance", cex = 1.8,
     adj = c(1.1, -0.5), xpd = TRUE)
abline(h = 0.1, col = "gray50", lwd = 2)

par(las = 1, mgp = c(1.5, 0.75, 0))#, xpd = TRUE)
axis(side = 1, at = 1:max_x, labels = ifelse(1:14 %% 2 == 1, 1:14, ""),
     col = "gray50", col.axis = "gray50",
     cex.axis = 1.5, lwd = 1, tcl = -0.25, lwd.ticks = 2)
axis(side = 2, at = seq(0, 1, length.out = 6),
     col = "gray50", col.axis = "gray50",
     cex.axis = 1.5, lwd = 1, tcl = -0.25, lwd.ticks = 2)
dev.off
```
# Method 2 Visualisation of  Nonlinearity
```{r}

a<-ggplot(dimred2021_knn250_2) +geom_point(aes(x=iso1,y=SH.STA.MMRT),color=col_pal[1])+ theme_bw() +ylab("Maternal mortality ratio \n(per 100 k live births)")+ xlab("Isomap dimension 1")
b<-ggplot(dimred2021_knn250_2) +geom_point(aes(x=iso1,y=NY.GDP.PCAP.CD),color=col_pal[2])+ theme_bw() +ylab(wdi_explained[wdi_explained$`Series Code`%in% "NY.GDP.PCAP.CD","Indicator Name"])+ xlab("Isomap dimension 1")+scale_y_continuous(position = "right")

library(gridExtra)
grid.arrange(a,b)# %>%  ggsave(file="../figures/06_plots/b_methods_nonlinear_pattern.png",device = "png")
```
# Method 3 Visualtising the linearity

```{r}
library(gridExtra)
c<-dimred2021_knn250_2 %>% ggplot()+geom_point(aes(y=iso1,x=PC1),color=col_pal[7])+ theme_bw() + xlab("Principle Component 1") +ylab("Isomap dimension 1")
d<-dimred2021_knn250_2 %>% ggplot()+geom_point(aes(y=NY.GDP.PCAP.CD,x=PC1),color=col_pal[5])+ theme_bw() + xlab("Principle Component 1") +ylab(wdi_explained[wdi_explained$`Series Code`%in% "NY.GDP.PCAP.CD","Indicator Name"])+scale_y_continuous(position = "right")
cowplot::plot_grid(a,b,c,d,cols = 2 ,align = "h",
          labels = c("B", "C", "D","E"))#%>%  ggsave(file="../figures/07_methods_linear.png",device = "png")
```


# Results 1 Shapley values and importance of nonlinear prediction of Xgboost 



```{r}
doMC::registerDoMC(50)

Xgboostinput<- fread("../results/06_Dimred_Exmort__withpc_monthly.csv")
Label<-"p_value_mort"
features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7","iso8","iso9","iso9","iso10")


custom_control <- trainControl(
  method = "cv",
  number = 12,  # Increase the number of folds
  search = "random",
  allowParallel = TRUE
  
)

param_grid <- expand.grid(
  nrounds = c(50,100,200),
  eta = c(0.05,0.3),
  max_depth = c(2,3, 4),
  colsample_bytree = c( 1),
  subsample = c(0.9),
  gamma = c(0),
  min_child_weight = c(0)
)
set.seed(123)
# Set the proportion for the test set (e.g., 0.2 for 20% test data)

test_proportion <- 1
Xgboostinput_filtered<-Xgboostinput %>%  select(features,Label) %>% drop_na()
Xgboostinput_filtered$p_value_mort<-Xgboostinput_filtered$p_value_mort %>% scale()
index <- caret::createDataPartition(Xgboostinput_filtered$iso1, p = test_proportion, list = FALSE)
# Split the data into training and test sets
train_data <- Xgboostinput_filtered[index, ]
test_data <- Xgboostinput_filtered[-index, ]


# Train the XGBoost model using the function
xgb_caret <- caret::train(
    x = train_data %>% dplyr::select(features),
    y = train_data %>% pull(Label),
    method = "xgbTree",
    verbosity = 1,
    trControl = custom_control,tuneGrid=param_grid,
    tuneLength = tunelength
  )
  
finalmodel <- caret::train(
    x = train_data %>% dplyr::select(features),
    y = train_data %>% pull(Label),
    method = "xgbTree",
    verbosity = 0,
    trControl = trainControl(method = "cv", number = 12),
    tuneGrid = xgb_caret$bestTune
  )

# Assuming you have the 'finalmodel' and 'test_data' from the previous steps
# predictedValue<-postResample(
#       predict(trainedModel, 
#           test_data %>% dplyr::select(features)),  
#         test_data %>% pull(Label)
#       )
# predictedValue

finalmodel


```


```{r}
library(shapper)

t <- xgb.plot.shap.summary(data = as.matrix(train_data %>% select(features)), model = finalmodel$finalModel, top_n = 12)
t$data %>% group_by(feature) %>% summarise(numberobservations=length(shap_value),mean=mean(abs(shap_value))) %>%  arrange(desc(mean))->asd
asd
asd$mean %>%  sum()
```


```{r}
# Get SHAP values
shap_values <- shap::shap_values(
  xgb_model = finalmodel$finalModel,
  X = as.matrix(train_data %>% select(features))
)

plota<-xgboost::xgb.ggplot.shap.summary(data = as.matrix(train_data %>% 
  dplyr::select(features)),
  model = finalmodel$finalModel) +scale_color_viridis("Feature Value")+ theme_bw() + xlab("Features")+ylab("Shapley value")+theme(legend.position = "left")

plot_grid(plota, plotb, labels = "A",rel_widths =   c(4/5, 1/5))
#ggsave("../figures/06_plots/ResultsA_nonlinearexmort.png",device = "png")


```


# Results 1 Shapley values and importance of nonlinear prediction of Xgboost 



# Resuls 1 B PCA
```{r}
Xgboostinput<- fread("../results/06_Dimred_Exmort__withpc_monthly.csv")
```


```{r}
Label<-"p_value_mort"
features<-c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14","PC15","PC16","PC17","PC18","PC19","PC20")

set.seed(123)
custom_control <- trainControl(
  method = "cv",
  number = 12,  # Increase the number of folds
  search = "random",
  allowParallel = TRUE

)
# custom_control <- trainControl(
#   method = "cv",                # cross-validation
#   number = 30,                   # number of folds
#   verboseIter = FALSE,           # print training progress
#   returnData = FALSE,
#   returnResamp = "all",         # saves losses across all models
#   classProbs = FALSE,           # no probabilities
#   summaryFunction = defaultSummary,  # regression metrics
#   allowParallel = TRUE          # use parallel backend if available
# )

param_grid <- expand.grid(
  nrounds = c(50,100),
  eta = c(0.05),
  max_depth = c(2,3, 4),
  colsample_bytree = c( 1),
  subsample = c(0.9),
  gamma = c(0),
  min_child_weight = c(0)
)
# param_grid <- expand.grid(
#   nrounds = c(100, 150),
#   eta = c(0.01, 0.03, 0.05),
#   max_depth = c(3, 4, 5),
#   gamma = c(0, 0.1, 0.2),
#   colsample_bytree = c(0.7, 0.8),
#   min_child_weight = c(1, 2),
#   subsample = c(0.7, 0.8)
# )

# Set the proportion for the test set (e.g., 0.2 for 20% test data)

test_proportion <- 0.8
Xgboostinput_filtered<-Xgboostinput %>%  select(features,Label) %>% scale() %>%  data.table()%>% drop_na()
index <- caret::createDataPartition(Xgboostinput_filtered$PC1, p = test_proportion, list = FALSE)
# Split the data into training and test sets
train_data <- Xgboostinput_filtered[index, ]
test_data <- Xgboostinput_filtered[-index, ]


# Train the XGBoost model using the function
xgb_caret <- caret::train(
    x = train_data %>% dplyr::select(features),
    y = train_data %>% pull(Label),
    method = "xgbTree",
    verbosity = 0,
    trControl = custom_control,tuneGrid=param_grid,
    tuneLength = tunelength
  )
  
finalmodel <- caret::train(
    x = train_data %>% dplyr::select(features),
    y = train_data %>% pull(Label),
    method = "xgbTree",
    verbosity = 0,
    trControl = trainControl(method = "cv", number = 12),
    tuneGrid = xgb_caret$bestTune)
# Assuming you have the 'finalmodel' and 'test_data' from the previous steps
predictedValue<-postResample(
      predict(finalmodel, 
          test_data %>% dplyr::select(features)),  
        test_data %>% pull(Label)
      )


```

```{r}
plota<-xgboost::xgb.ggplot.shap.summary(data = as.matrix(train_data %>% 
  dplyr::select(features)),
  model = finalmodel$finalModel) +scale_color_viridis("Feature Value")+ theme_bw() + xlab("Features")+ylab("Shapley value")+theme(legend.position = "bottom")
plota
```

```{r}

t <- xgb.plot.shap.summary(data = as.matrix(train_data %>% select(features)), model = finalmodel$finalModel, top_n = 12)
t$data %>% group_by(feature) %>% summarise(numberobservations=length(shap_value),mean=mean(abs(shap_value))) %>%  arrange(desc(mean))->asd

asd$mean %>%  sum()
```


```{r}
importance_matrix <- xgb.importance(feature_names = colnames(features), model = trainedModel$finalModel)
# importance_matrix %>% fwrite("../data/07_importance_PCA.csv")
c<-xgb.ggplot.importance(importance_matrix) +
  geom_col(fill="slategrey")+
  ylab("")+theme_classic()+theme() +theme(legend.position = "asd") +ggtitle("Importance")

apply(importance_matrix[,c(2,3,4)],2,sum)
plot_grid(plota, c, labels = c("A","B"),rel_widths =   c(4/5, 1.5/5))+ggtitle("sada")
#ggsave("../figures/06_plots/ResultsA_nonlinearexmort.png",device = "png")


```

# Results 2 WDI or Dimred How predicts better bootstrap


### impute WDI
```{r}

shrinkWDI<-dimred2021_knn250_2[,colnames(dimred2021_knn250_2)%in% c("Country Code","year",wdi_explained$`Series Code`),with=F] %>% filter(year<2020)

aTable<-shrinkWDI[,"Country Code"] %>%  unique
for(i in 3:ncol(shrinkWDI)){
  look<-shrinkWDI[,colnames(shrinkWDI)[c(1,2,i)],with=F] 
  colnames(look)[3]<-"value"
  best<-look[,.(value=last(value),year=last(year)),by=`Country Code`] %>%  filter(!is.na(value))
  okaisch<-look[!(look$`Country Code`%in% best$`Country Code`)&year%in% c(2000:2020),.(value=mean(value,na.rm=T),sd=var(value,na.rm=T)),by=`Country Code`]
  
  thisone<-rbind(best[,c("Country Code","value")],okaisch[,c("Country Code","value")])
  colnames(thisone)[2]<-colnames(shrinkWDI)[c(i)]
  aTable<-left_join(aTable,thisone, by = "Country Code")
}

bTable<-aTable[,-1] %>% sapply( . ,function(x)(as.numeric(x))) %>% data.frame() 

cTable<-sapply(bTable, function(x)x<-if_else(is.nan(x),999,x)) %>% data.frame()

cTable[cTable==999]<-NA
df_filled <- pca(object =  cTable, method = "ppca")
table2019_wdiImputed<-left_join(cbind(aTable[,1],data.frame(df_filled@completeObs)),dimred2021_knn250_2[dimred2021_knn250_2$year%in%2019,c("Country Code","pscore_mean","year")],by = "Country Code")
table2020_wdiImputed<-left_join(cbind(aTable[,1],data.frame(df_filled@completeObs)),dimred2021_knn250_2[dimred2021_knn250_2$year%in%2020,c("Country Code","pscore_mean","year")],by = "Country Code")
```



```{r}
table2019_wdiImputed2<-table2019_wdiImputed %>% mutate(identifier=paste0(`Country Code`,"2020"))
table2020_wdiImputed2<-table2020_wdiImputed %>% mutate(identifier=paste0(`Country Code`,"2021"))
WDI_2020_2021<-rbind(table2019_wdiImputed2,table2020_wdiImputed2)

excessCounrty_monthly_pvalue2<-excessCounrty_monthly_pvalue  %>% mutate(identifier=paste0(iso3,year))

```
#### WDI XGBOOST
```{r}
Xgboostinput_first<-merge(WDI_2020_2021,excessCounrty_monthly_pvalue2 %>% select(p_value_mort,identifier),by = "identifier")

Label<-"p_value_mort"
features<-colnames(Xgboostinput_first)[colnames(Xgboostinput_first) %in% wdi_explained$`Series Code`]


```


```{r}
set.seed(123)
#library(doMC)
doMC::registerDoMC(30)
custom_control <- trainControl(
  method = "cv",
  number = 30,  # Increase the number of folds
  search = "random",
  allowParallel = TRUE
  
)

param_grid <- expand.grid(
  nrounds = c(50),
  eta = c(0.3),
  max_depth = c(3,6,9,12),
  colsample_bytree = c( 1),
  subsample = c(1),
  gamma = c(0),
  min_child_weight = c(1,3,5)
)
param_grid <- expand.grid(
  nrounds = 100,
  eta = c(0.01, 0.05, 0.1, 0.3),
  max_depth = c(3, 6, 9, 12),
  gamma = c(0, 0.1, 1, 2),
  colsample_bytree = c(0.5, 0.7, 0.9),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.5, 0.7, 0.9)
  # nthread: Set this as per your requirement or omit to use all available cores.
)


# Set the proportion for the test set (e.g., 0.2 for 20% test data)

test_proportion <- 0.7
Xgboostinput_filtered<-Xgboostinput_first %>%  select(features,Label) %>% drop_na()
index <- caret::createDataPartition(Xgboostinput_filtered$p_value_mort, p = test_proportion, list = FALSE)
# Split the data into training and test sets
train_data <- Xgboostinput_filtered[index, ]
test_data <- Xgboostinput_filtered[-index, ]


# Train the XGBoost model using the function
thisData_2_input <- train_data %>% select(Label, features) %>% drop_na()
  
  
  # Test different parameters
# xgb_caret_wdi_imputed <- caret::train(
#   x = thisData_2_input %>% dplyr::select(features),
#   y = thisData_2_input %>% pull(Label),
#   method = "xgbTree",
#   verbosity = 1,
#   trControl = custom_control,tuneGrid=param_grid,
#   tuneLength = tunelength
# )
#save(xgb_caret_wdi_imputed,file = "../output/07_results2_imputedWDI_trained")
load("../output/07_results2_imputedWDI_trained")
# finalmodel_wdi_imputed <- caret::train(
#   x = thisData_2_input %>% dplyr::select(features),
#   y = thisData_2_input %>% pull(Label),
#   method = "xgbTree",
#   verbosity = 0,
#   trControl = trainControl(method = "cv", number = 30),
#   tuneGrid = xgb_caret$bestTune
# )
#save(finalmodel_wdi_imputed,file = "../output/07_results2_imputedWDI_final")  

load("../output/07_results2_imputedWDI_final")
finalmodel
predictedValue<-postResample(
      predict(finalmodel, 
          test_data %>% dplyr::select(features)),  
        test_data %>% pull(Label)
      )
predictedValue
```

```{r}
xgboost::xgb.ggplot.shap.summary(data = as.matrix(train_data %>% 
  dplyr::select(features)),
  model = finalmodel$finalModel) +scale_color_viridis("Feature Value")+ theme_bw() + xlab("Features")+ylab("Shapley value")+theme(legend.position = "left")


xgboost::xgb.plot.shap(
  data = as.matrix(train_data %>% 
  dplyr::select(features)),
  model = trainedModel$finalModel,
  top_n = 9,
  n_col = 3
)
importance_matrix <- xgb.importance(feature_names = colnames(features), model = finalmodel$finalModel)
importance_matrix$name<-wdi_explained[wdi_explained$`Series Code`%in% importance_matrix$Feature,"Indicator Name"]
#importance_matrix %>% fwrite("../data/07_wdiImputed_importance_PCA.csv")
c<-xgb.ggplot.importance(importance_matrix) +
  geom_col(fill="slategrey")+
  ylab("")+theme_classic()+theme() +theme(legend.position = "asd") +ggtitle("Importance")

```




### Dimred ISO


```{r}
Xgboostinput<- fread("../results/06_Dimred_Exmort__withpc_monthly.csv")
Xgboostinput<-Xgboostinput %>% mutate(yearmon=as.numeric(as.factor(str_remove(pattern = "[a-zA-Z_]+",identifier))))
```




```{r}
Label<-"p_value_mort"
features<-c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14")
features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7")
require(Boruta)

```
#### feature selection and tuning
```{r}
library(doMC)
registerDoMC(30)
custom_control <- trainControl(
  method = "cv",
  number = 30,  # Increase the number of folds
  search = "random",
  allowParallel = TRUE
  
)

# param_grid <- expand.grid(
#   nrounds = c(50,100,200),
#   eta = c(0.05,0.1,0.3),
#   max_depth = c(3, 4,6),
#   gamma = c(0, 0.1, 1, 2),
#   colsample_bytree = c(0.5, 0.7, 0.9),
#   min_child_weight = c(1, 3, 5),
#   subsample = c(0.5, 0.7, 0.9)
# )
param_grid <- expand.grid(
  nrounds = c(100),
  eta = c(0.3),
  max_depth = c(3, 4,6),
  gamma = c(0.1, 1, 2),
  colsample_bytree = c( 0.9),
  min_child_weight = c(1),
  subsample = c( 0.7)
)

features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7","iso8","iso9","iso10","iso11","iso12","iso13","iso14","iso15","iso16","iso17","iso18","iso19","iso20")


set.seed(123) #this seed is for reproducing predictions
model_ffs_iso <- ffs(predictors = Xgboostinput %>% select(features),
response = Xgboostinput %>%  pull(Label),
metric = "Rsquared",
method = "xgbTree",
verbosity = 0,
trControl = custom_control,
tuneGrid = param_grid)

model_ffs_iso$results-> asd
saveRDS(model_ffs_iso, file = "output/07_alliso_ffs_iso.rds")


require(CAST)

features<-c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14","PC15","PC16","PC17","PC18","PC19","PC20")

# set.seed(123) #this seed is for reproducing predictions
# model_ffs_pca <- ffs(predictors = Xgboostinput %>% select(features),
# response = Xgboostinput %>%  pull(Label),
# metric = "Rsquared",
# method = "xgbTree",
# verbosity = 0,
# trControl = trainControl(method ="cv",allowParallel = T),
# tuneGrid = param_grid)
# model_ffs_pca$selectedvars
# saveRDS(model_ffs_pca, file = "output/07_ffs_PCA.rds")
```


```{r}
require(CAST)
features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7","iso8","iso9","iso10","iso11","iso12","iso13","iso14","iso15","iso16","iso17","iso18","iso19","iso20")

set.seed(123) #this seed is for reproducing predictions
model_ffs_iso <- ffs(predictors = Xgboostinput %>% select(features),
response = Xgboostinput %>%  pull(Label),
metric = "Rsquared",
method = "xgbTree",
verbosity = 0,
trControl = costum_control,
tuneGrid = param_grid)

model_ffs_iso$results-> asd
saveRDS(model_ffs_iso, file = "output/07_ffs_iso.rds")
```


```{r}
model_ffs$selectedvars
```







```{r}
set.seed(123)
doMC::registerDoMC(30)


# Set the proportion for the test set (e.g., 0.2 for 20% test data)

test_proportion <- 0.8
Xgboostinput_filtered<-Xgboostinput %>%  select(features,Label) %>% drop_na()
index <- caret::createDataPartition(Xgboostinput_filtered$p_value_mort, p = test_proportion, list = FALSE)
# Split the data into training and test sets
train_data <- Xgboostinput_filtered[index, ]
test_data <- Xgboostinput_filtered[-index, ]


  
  # Test different parameters
xgb_caret_iso <- caret::train(
  x = train_data %>% dplyr::select(features),
  y = train_data %>% pull(Label),
  method = "xgbTree",
  verbosity = 1,
  trControl = custom_control,tuneGrid=param_grid,
  tuneLength = tunelength
)
save(xgb_caret_iso,file = "../output/07_results2_iso_trained")
finalmodel_iso <- caret::train(
  x = train_data %>% dplyr::select(features),
  y = train_data %>% pull(Label),
  method = "xgbTree",
  verbosity = 0,
  trControl = trainControl(method = "cv", number = 30),
  tuneGrid = xgb_caret_iso$bestTune
)
save(finalmodel_iso,file = "../output/07_results2_iso_final")  



# Assuming you have the 'finalmodel' and 'test_data' from the previous steps
predictedValue<-postResample(
      predict(finalmodel_iso, 
          test_data %>% dplyr::select(features)),  
        test_data %>% pull(Label)
      )
predictedValue


```

```{r}
# xgboost::xgb.ggplot.shap.summary(data = as.matrix(train_data %>% 
#   dplyr::select(features)),
#   model = finalmodel_$finalModel) +scale_color_viridis("Feature Value")+ theme_bw() + xlab("Features")+ylab("Shapley value")+theme(legend.position = "left")

```

```{r}

features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7","iso8","iso9","iso10","iso11","iso12","iso13","iso14","iso15","iso16","iso17","iso18","iso19","iso20")

set.seed(123) #this seed is for reproducing predictions
model_ffs_iso <- ffs(predictors = Xgboostinput %>% select(features),
response = Xgboostinput %>%  pull(Label),
metric = "Rsquared",
method = "xgbTree",
verbosity = 0,
trControl = custom_control,
tuneGrid = param_grid)

model_ffs_iso$results-> asd
saveRDS(model_ffs_iso, file = "output/07_ffs_iso.rds")



```



### PCA
```{r}
Xgboostinput<- fread("../results/06_Dimred_Exmort__withpc_monthly.csv")
Xgboostinput<-Xgboostinput %>% mutate(yearmon=as.numeric(as.factor(str_remove(pattern = "[a-zA-Z_]+",identifier)))) 


```
#### feature selection


```{r}

set.seed(123)

Label<-"p_value_mort"
features1<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7","iso8","iso9","iso10","iso11","iso12","iso13","iso14","iso15","iso16","iso17","iso18","iso19","iso20")
features2<-c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14","PC15","PC16","PC17","PC18","PC19","PC20")
#features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7")
features<-features2
set.seed(123)
doMC::registerDoMC(50)
custom_control <- trainControl(
  method = "cv",
  number = 6,
  allowParallel = TRUE
  
)

param_grid <- expand.grid(
  nrounds = c(50,100,200),
  eta = c(0.2,0.3),
  max_depth = c(3, 4,6),
  gamma = c(0, 0.1, 1, 2),
  colsample_bytree = c(0.5, 0.7, 0.9),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.5, 0.7, 0.9)
)

# Set the proportion for the test set (e.g., 0.2 for 20% test data)

test_proportion <- 0.8
Xgboostinput_filtered<-Xgboostinput %>%  select(features,Label) %>% drop_na()
index <- caret::createDataPartition(Xgboostinput_filtered$p_value_mort, p = test_proportion, list = FALSE)
# Split the data into training and test sets
train_data <- Xgboostinput_filtered[index, ]
test_data <- Xgboostinput_filtered[-index, ]


  
  # Test different parameters
xgb_caret_pca <- caret::train(
  x = train_data %>% dplyr::select(features),
  y = train_data %>% pull(Label),
  method = "xgbTree",
  verbosity = 1
)

#save(xgb_caret_pca,file = "../output/07_results3_pca_trained")
finalmodel_pca <- caret::train(
  x = train_data %>% dplyr::select(features),
  y = train_data %>% pull(Label),
  method = "xgbTree",
  verbosity = 0,
  trControl = trainControl(method = "cv", number = 6),
  tuneGrid = xgb_caret_pca$bestTune
)

#save(finalmodel_pca,file = "../output/07_results3_pca_final")  



# Assuming you have the 'finalmodel' and 'test_data' from the previous steps
predictedValue<-postResample(
      predict(finalmodel_pca, 
          test_data %>% dplyr::select(features)),  
        test_data %>% pull(Label)
      )
predictedValue


```


```{r}
xgboost::xgb.ggplot.shap.summary(data = as.matrix(train_data %>% 
  dplyr::select(features)),
  model = finalmodel_pca$finalModel) +scale_color_viridis("Feature Value")+ theme_bw() + xlab("Features")+ylab("Shapley value")+theme(legend.position = "left")


xgboost::xgb.plot.shap(
  data = as.matrix(train_data %>% 
  dplyr::select(features)),
  model = finalmodel_pca$finalModel,
  top_n = 9,
  n_col = 3
)
importance_matrix <- xgb.importance(feature_names = colnames(features), model = finalmodel$finalModel)
importance_matrix$name<-wdi_explained[wdi_explained$`Series Code`%in% importance_matrix$Feature,"Indicator Name"]
#importance_matrix %>% fwrite("../data/07_wdiImputed_importance_PCA.csv")
c<-xgb.ggplot.importance(importance_matrix) +
  geom_col(fill="slategrey")+
  ylab("")+theme_classic()+theme() +theme(legend.position = "asd") +ggtitle("Importance")

```

```{r}

# require(CAST)
# 
# features<-c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14","PC15","PC16","PC17","PC18","PC19","PC20")
# 
# set.seed(123) #this seed is for reproducing predictions
# model_ffs_pca <- ffs(predictors = Xgboostinput %>% select(features),
# response = Xgboostinput %>%  pull(Label),
# metric = "Rsquared",
# method = "xgbTree",
# verbosity = 0,
# trControl = trainControl(method ="cv",allowParallel = T),
# tuneGrid = param_grid)
# saveRDS(model_ffs_pca, file = "output/07_ffs_PCA.rds")
```


### plot both
```{r}

a<-fread("../results/06_wdi_19_to_pscoreMean_bootstrap_1000.csv")
b<-fread("../results/06_dimred_19_to_pscoreMean_bootstrap_1000.csv")
a$Dataset<-"WDI imputed"
b$Dataset<-"Latent Space"
plotR<-rbind(a,b) %>% ggplot(aes(x=Rsquared,fill=Dataset))+geom_histogram(bins = 100)+theme_classic()+xlim(0,1)+scale_fill_manual(values = col_pal)+theme(legend.position = "sad")+ylab("N")+xlab(expression(paste("R"^2)))
plotRMS<-rbind(a,b) %>% ggplot(aes(x=RMSE,fill=Dataset))+geom_histogram(bins = 100)+theme_classic()+xlim(5,15)+scale_fill_manual(values = col_pal)+theme(legend.position = "sad")+ylab("")

plot_bootstrap<-ggarrange(plotR,plotRMS)
plot_bootstrap_legend<- rbind(a,b) %>% ggplot(aes(x=RMSE,fill=Dataset))+geom_histogram(bins = 100)+scale_fill_manual(values = col_pal)
plot_bootstrap
#plot_bootstrap%>%  ggsave(file="../figures/06_plots/c_compare_prediction_bootstrap.png",device = "png",width = 9,height = 4)
plot_bootstrap_legend%>%  ggsave(file="../figures/06_plots/c_compare_prediction_bootstrap_legend.png",device = "png")

```
# Result 3 Monthly excess mort
## first round 2019 dimred
```{r}
features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7")
dimredData<-dimred2021_knn250_2[] %>% filter(year%in% 2019) %>% select(c(`Country Code`,features)) %>%  filter(!is.na(iso1))
excessmonthly2<-merge(excessmonthly %>% select(date,`Country Code`,p_value_mort),dimredData,by="Country Code")
listofmonth<-excessmonthly$date %>%  unique 
listofmonth<-listofmonth[!is.na(listofmonth)] 
require(xgboost)
require(tidyverse)
require(data.table)
library(shapper)
require("SHAPforxgboost")
cooloutput<-data.frame()


features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7")
     label<-"p_value_mort"

     
     system.time(result_bootstrap_dimred <- for(i in 1:length(listofmonth)){
    


    
    
    
    thismonth<-listofmonth[i]
    excessmonthly2 %>%  filter(date%in% thismonth) %>% select(all_of(c(features,label)))-> runthismonthlysubset
   
    trainIndex <- createDataPartition(runthismonthlysubset$p_value_mort, p=0.8, list=FALSE)
    train_df <- runthismonthlysubset[trainIndex,]
    test_df <- runthismonthlysubset[-trainIndex,]

    a<-train_df %>% dplyr::select(all_of(features)) %>% as.matrix()
    xgb_caret_dimred <- caret::train(x = train_df %>% dplyr::select(all_of(features)),
                   y = train_df %>% pull(label),
                   method = "xgbTree",
                   verbosity = 0,trControl=custom_control,tuneGrid = param_grid
                   ,tuneLength = 10
    
                    )
    
    # xgb_caret_dimred
    # mod <- xgboost::xgboost(data = a, 
    #                     label = runthismonthlysubset$p_value_mort, 
    #                     params = param_list, nrounds = 10,
    #                     verbose = FALSE,
    #                     early_stopping_rounds = 8)
    
    finalmodel_dimred <- caret::train(x = train_df %>% dplyr::select(all_of(features)),
                   y = train_df %>% pull(label),
                   method = "xgbTree",
                   verbosity = 0,
                   ## CAST CV LLO (Hanna Meyer)
                   trControl=trainControl(method="cv",
                                          number = 60),
                   # early_stopping_rounds = 10,
                   tuneGrid = xgb_caret_dimred$bestTune)
    
    importance_matrix<-varImp(finalmodel_dimred)$importance %>%
         mutate(var = rownames(varImp(xgb_caret_dimred)$importance))%>%
         dplyr::rename(importance = Overall) 
    importance_matrix$name<- thismonth
    importance_matrix$rsquared<-finalmodel_dimred$results$Rsquared
    predictedValue<-postResample(
      predict(finalmodel_dimred, 
          test_df %>% dplyr::select(-label)),  
        test_df %>% pull(label)
      )
    
    shap_values <- shap.values(xgb_model = finalmodel_dimred$finalModel, X_train = as.matrix(train_df %>% dplyr::select(all_of(features))))
    shap_values
    # The ranked features by mean |SHAP|
    shaplmean<-data.frame(Shapl_feature=shap_values$mean_shap_score) %>% rownames_as_column(var="var")
    
    
      
    importance_matrix$rsquared_tested<-predictedValue[2]
    importance_matrix$rmse_tested<-predictedValue[1]
    importance_matrix<-cbind(importance_matrix,shaplmean[match(shaplmean$var,importance_matrix$var),])
    # shap_values <- shap.values(xgb_model = mod, X_train = a)
    # # The ranked features by mean |SHAP|
    # shaploutput<-shap_values$mean_shap_score 
    # 
    # orderShap<-match(rownames(importance_matrix), names(shaploutput))
    # 
    # importance_matrix$shapmean<-shaploutput[orderShap]
    cooloutput<-rbind(cooloutput,importance_matrix)
    
})

```
```{r}

rsquared_values_plot<-
  cooloutput %>% select(name,rsquared_tested) %>%  unique %>% ggplot( . , aes(x=name, y=rsquared_tested)) +
  geom_segment( aes(x=name, xend=name, y=0, yend=rsquared_tested), color="grey") +
  geom_point( color=col_pal[5], size=4) +
  theme_light() + geom_line()+
  theme(
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank()
  ) +
  xlab("") +
  ylab("Value of Y") +ylim(0.2,1)
# heatmap
rsquared_values_plot
```


```{r}


start_date<- as.Date("2020-1-1")
end_date<- as.Date("2021-12-1")
# Heatmap 
cooloutput$var1<-cooloutput$var
cooloutput$var<-NULL
excessmonthy <- excessCounrty_monthly_pvalue %>% arrange(country,year,month) %>% rename(`Country Code`=iso3) %>% 
  mutate(date = as.Date(paste(year, month, "01", sep = "-"), format = "%Y-%m-%d"),identifier=paste0(`Country Code`,"_",year(date),"_",month(date)))%>% filter(date>"2020-02-01") %>% merge( . , thisisOWID,by.x="identifier",by.y="monthyear")
excessmonthy <- excessCounrty_monthly_pvalue %>% arrange(country,year,month) %>% rename(`Country Code`=iso3) %>% 
  mutate(date = as.Date(paste(year, month, "01", sep = "-"), format = "%Y-%m-%d"),identifier=paste0(`Country Code`,"_",year(date),"_",month(date)))%>% filter(date>"2020-02-01") %>% merge( . , thisisOWID,by.x="identifier",by.y="monthyear")
importanceplot<-cooloutput %>% 
  mutate(var = factor(var, levels=rev(features))) %>% 
  ggplot( . , aes(name, var, fill= Shapl_feature)) + 
  geom_tile()+scale_fill_viridis()+theme_classic()+xlab("")+xlim(1,1)+
  scale_x_date(limits = c(start_date, end_date))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+theme(legend.position = "sad")

plot_grid(rsquared_values_plot,exmortelement, importanceplot,ncol = 1,rel_heights =     c(1.5/5, 2/5,3/5))

```

```{r}
postResample(
  predict(finalmodel, 
          test_df %>% dplyr::select(-share_exposed_pop)),  
  test_df %>% pull(share_exposed_pop)
  )

## nash-sutville efficiency
NSE(predict(finalmodel, 
          test_df %>% dplyr::select(-share_exposed_pop)), 
    test_df %>% pull(share_exposed_pop))
```


## second round 2020 and 2021

### looks horrible dont do it again
```{r}

features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7")
dimredData2020<-dimred2021_knn250_2[] %>% filter(year%in% 2020) %>% 
  mutate(identifier=paste0(`Country Code`,"_",year)) %>% 
  select(c(identifier,features)) %>%  filter(!is.na(iso1)) 
dimredData2021<-dimred2021_knn250_2[] %>% filter(year%in% 2021) %>% 
  mutate(identifier=paste0(`Country Code`,"_",year)) %>% 
  select(c(identifier,features)) %>%  filter(!is.na(iso1)) 
dimredDataboth<-rbind(dimredData2020,dimredData2021)
excessmonthly<-excessmonthly %>% 
  mutate(identifier=paste0(`Country Code`,"_",year))
excessmonthly2<-merge(excessmonthly,dimredDataboth,by="identifier")
listofmonth<-excessmonthly$date %>%  unique 
listofmonth<-listofmonth[!is.na(listofmonth)] 
require(xgboost)
require(tidyverse)
require(data.table)
library(shapper)
require("SHAPforxgboost")
cooloutput_2020_2021<-data.frame()

system.time(result_bootstrap_dimred <- for(i in 1:length(listofmonth)){
  



    
    features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7")
    thismonth<-listofmonth[i]
    excessmonthly2 %>%  filter(date%in% thismonth) -> runthismonthlysubset
    label<-"pscore_mean"
    
    a<-runthismonthlysubset %>% dplyr::select(all_of(features)) %>% as.matrix()
    xgb_caret_dimred <- caret::train(x = runthismonthlysubset %>% dplyr::select(all_of(features)),
                   y = runthismonthlysubset %>% pull("p_value_mort"),
                   method = "xgbTree",
                   verbosity = 0,trControl=custom_control,tuneGrid = param_grid
                   ,tuneLength = 10
    
                    )
    
    # xgb_caret_dimred
    # mod <- xgboost::xgboost(data = a, 
    #                     label = runthismonthlysubset$p_value_mort, 
    #                     params = param_list, nrounds = 10,
    #                     verbose = FALSE,
    #                     early_stopping_rounds = 8)
    
    finalmodel_dimred <- caret::train(x = runthismonthlysubset %>% dplyr::select(all_of(features)),
                   y = runthismonthlysubset %>% pull(p_value_mort),
                   method = "xgbTree",
                   verbosity = 0,
                   ## CAST CV LLO (Hanna Meyer)
                   trControl=trainControl(method="cv",
                                          number = 30),
                   # early_stopping_rounds = 10,
                   tuneGrid = xgb_caret_dimred$bestTune)
    
    importance_matrix<-varImp(finalmodel_dimred)$importance %>%
         mutate(var = rownames(varImp(xgb_caret_dimred)$importance))%>%
         dplyr::rename(importance = Overall) 
    importance_matrix$name<- thismonth
    importance_matrix$rsquared<-finalmodel_dimred$results$Rsquared
    
    # shap_values <- shap.values(xgb_model = mod, X_train = a)
    # # The ranked features by mean |SHAP|
    # shaploutput<-shap_values$mean_shap_score 
    # 
    # orderShap<-match(rownames(importance_matrix), names(shaploutput))
    # 
    # importance_matrix$shapmean<-shaploutput[orderShap]
    cooloutput_2020_2021<-rbind(cooloutput_2020_2021,importance_matrix)
    
})
```
```{r}

rsquared_values_plot<-
  cooloutput_2020_2021 %>% select(name,rsquared) %>%  unique %>% ggplot( . , aes(x=name, y=rsquared)) +
  geom_segment( aes(x=name, xend=name, y=0, yend=rsquared), color="grey") +
  geom_point( color=col_pal[5], size=4) +
  theme_light() + geom_line()+
  theme(
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank()
  ) +
  xlab("") +
  ylab("Value of Y") +ylim(0.2,1)
# heatmap
```


```{r}


start_date<- as.Date("2020-1-1")
end_date<- as.Date("2021-12-1")
# Heatmap 
importanceplot<-cooloutput_2020_2021 %>% filter(!is.na(var))%>% 
  mutate(var = factor(var, levels=rev(features))) %>% 
  ggplot( . , aes(name, var, fill= importance)) + 
  geom_tile()+scale_fill_viridis()+theme_classic()+xlab("")+xlim(1,1)+
  scale_x_date(limits = c(start_date, end_date))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+theme(legend.position = "sad")

plot_grid(rsquared_values_plot,exmortelement, importanceplot,ncol = 1,rel_heights =     c(1.5/5, 2/5,3/5))

```
## third round 2019 dimred on clustes
### the clusters
```{r}
clusterbyExcessandIsos<-dimred2021_knn250_2 %>% filter(year%in% 2019,`Country Code`!="PER") %>%  select(`Country Code`,pscore_mean,features) %>%  drop_na()
kmeans_result <- kmeans(clusterbyExcessandIsos[,-1], centers = 5)
clusterbyExcessandIsos$Cluster<-kmeans_result$cluster
```

```{r}
theclusteredFrame<-data.frame()
for(i in unique(clusterbyExcessandIsos$Cluster))
  
  
  

var_caountry<-clusterbyExcessandIsos[clusterbyExcessandIsos$Cluster %in% i,] %>% select(`Country Code`) %>%  unlist()
features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7")
dimredData<-dimred2021_knn250_2[] %>% filter(year%in% 2019) %>% filter(`Country Code`%in% var_caountry) %>% select(c(`Country Code`,features)) %>%  filter(!is.na(iso1))
excessmonthly2<-merge(excessmonthly %>% select(date,`Country Code`,p_value_mort),dimredData,by="Country Code")

listofmonth<-excessmonthly$date %>%  unique 
listofmonth<-listofmonth[!is.na(listofmonth)] 
require(xgboost)
require(tidyverse)
require(data.table)
library(shapper)
require("SHAPforxgboost")
cooloutput<-data.frame()

system.time(result_bootstrap_dimred <- for(i in 1:length(listofmonth)){
  

    
    features<-c("iso1","iso2","iso3","iso4","iso5","iso6","iso7")
    thismonth<-listofmonth[i]
    excessmonthly2 %>%  filter(date%in% thismonth) -> runthismonthlysubset
    label<-"pscore_mean"
    
    a<-runthismonthlysubset %>% dplyr::select(all_of(features)) %>% as.matrix()
    xgb_caret_dimred <- caret::train(x = runthismonthlysubset %>% dplyr::select(all_of(features)),
                   y = runthismonthlysubset %>% pull("p_value_mort"),
                   method = "xgbTree",
                   verbosity = 0,trControl=custom_control,tuneGrid = param_grid
                   ,tuneLength = 10
    
                    )
    
    # xgb_caret_dimred
    # mod <- xgboost::xgboost(data = a, 
    #                     label = runthismonthlysubset$p_value_mort, 
    #                     params = param_list, nrounds = 10,
    #                     verbose = FALSE,
    #                     early_stopping_rounds = 8)
    
    finalmodel_dimred <- caret::train(x = runthismonthlysubset %>% dplyr::select(all_of(features)),
                   y = runthismonthlysubset %>% pull(p_value_mort),
                   method = "xgbTree",
                   verbosity = 0,
                   ## CAST CV LLO (Hanna Meyer)
                   trControl=trainControl(method="cv",
                                          number = 60),
                   # early_stopping_rounds = 10,
                   tuneGrid = xgb_caret_dimred$bestTune)
    
    importance_matrix<-varImp(finalmodel_dimred)$importance %>%
         mutate(var = rownames(varImp(xgb_caret_dimred)$importance))%>%
         dplyr::rename(importance = Overall) 
    importance_matrix$name<- thismonth
    importance_matrix$rsquared<-finalmodel_dimred$results$Rsquared
    
    # shap_values <- shap.values(xgb_model = mod, X_train = a)
    # # The ranked features by mean |SHAP|
    # shaploutput<-shap_values$mean_shap_score 
    # 
    # orderShap<-match(rownames(importance_matrix), names(shaploutput))
    # 
    # importance_matrix$shapmean<-shaploutput[orderShap]
    cooloutput<-rbind(cooloutput,importance_matrix)
    
})
cooloutput$cluster<-i
theclusteredFrame<-bind_rows(theclusteredFrame,cooloutput)


```
```{r}

rsquared_values_plot<-
  cooloutput %>% select(name,rsquared) %>%  unique %>% ggplot( . , aes(x=name, y=rsquared)) +
  geom_segment( aes(x=name, xend=name, y=0, yend=rsquared), color="grey") +
  geom_point( color=col_pal[5], size=4) +
  theme_light() + geom_line()+
  theme(
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank()
  ) +
  xlab("") +
  ylab("Value of Y") +ylim(0.2,1)
# heatmap
```


```{r}


start_date<- as.Date("2020-1-1")
end_date<- as.Date("2021-12-1")
# Heatmap 
importanceplot<-cooloutput %>% filter(!is.na(var))%>% 
  mutate(var = factor(var, levels=rev(features))) %>% 
  ggplot( . , aes(name, var, fill= importance)) + 
  geom_tile()+scale_fill_viridis()+theme_classic()+xlab("")+xlim(1,1)+
  scale_x_date(limits = c(start_date, end_date))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+theme(legend.position = "sad")

plot_grid(rsquared_values_plot,exmortelement, importanceplot,ncol = 1,rel_heights =     c(1.5/5, 2/5,3/5))

```
